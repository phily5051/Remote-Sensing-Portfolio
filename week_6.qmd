---
title: "Week 6"
bibliography: references.bib
editor: visual
---

<h1>6 Classification methods 1</h1>

<h2>6.1 Summary</h2>

In this summary, I will go through several Machine Learning (ML) classification methods. The classification methods can be divided into two approach - unsupervised and supervised algorithms. However, this diary will mainly focus on the supervised algorithms.

Supervised ML follows the following process:

    - Class definition
    - Pre-processing
    - Training
    - Pixel assignment
    - Accuracy assessment

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/supervised_process.png')
```

Supervised Classification Process in Remote Sensing (Source: [GISgeography](https://gisgeography.com/supervised-unsupervised-classification-arcgis/))

<h3>6.1.1 **Classification and Regression Trees (CART)**</h3>

<h4>

(1) General Information about CART

    </h4>

**CART** is a binary split and it consists of *classification trees* and *regression trees*. My interpretation of CART is that when we put our data on a 2D graph, we can see that data points cannot be solely captured by a single straight line.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/cart_reg_decision.png')
```

CART Algorithm (Source: [DataDrivenInvestor](https://medium.datadriveninvestor.com/how-do-regression-trees-work-94999c5105d))

Thus, we subset our data into smaller groups and calculate the average for each data chunk. Each chunk is composed of some nodes. We add up nodes in the chunk one by one and check to what extent adding nodes gives the **lowest sum of squared residuals (SSR)**. To a certain point, adding nodes to the chunk would return a lower SSR, however, at some point we will notice that adding an extra node would give us a higher SSR. In this manner, We check SSR for different threshold till we get the lowest SSR. When we identify the lowest SSR, the average of the nodes in that chunk becomes a threshold. This threshold is where a decision tree divides. This indicates if we subset our data in many times, we can get a precise and accurate model. However, this poses a problem of over-fitting.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/cart_ssr.png')
```

CART SSR Threshold (Source: [DataDrivenInvestor](https://medium.datadriveninvestor.com/how-do-regression-trees-work-94999c5105d))

<h4>

(2) How to prevent over-fitting in CART?

    </h4>

There are two methods to prevent over-fitting in CART.

-   Setting Maximum Tree Depth: can limit how much trees can grow. 20 is default.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/cart_max_depth.png')
```

CART with Maximum Tree Depth (Source: [Towards Data Science](https://towardsdatascience.com/3-techniques-to-avoid-overfitting-of-decision-trees-1e7d3d985a09))

-   Weakest link pruning: basically means that we use a full size regression tree and we see if removing a leaf (a chunk of nodes) gives a lower tree score. We start with 0 for the value of $\alpha$, and increase it until we get a lower tree score. The equation for tree score is as below:

$Tree-score$ = $SSR$ + $\alpha$ \* $T(Number-of-leaves)$

Once we have those values for $\alpha$, we divide our data into training and test data. We take training data with all the values for $\alpha$ and calculate the SSR. Then, we look for which $\alpha$ value returns the lowest SSR. We repeat this process several times and look for the average value until we get the lowest SSR.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/cart_pruning.png')
```

CART with Pruning (Source: [Towards Data Science](https://towardsdatascience.com/3-techniques-to-avoid-overfitting-of-decision-trees-1e7d3d985a09))

<h3>6.1.2 **Random Forests (RF)**</h3>

<h4>

(1) General Information about RF

    </h4>

**RF** simply means that many are better than one. We do "*bootstrap*" samples (*bagging* - only 70% of data is used), and create nodes with random number of variables and on and on. Eventually, we will make many decision trees from random number of variables. These different decision trees are called a forest.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/rf_flowchart.jpg')
```

Random Forests Classification (Source: [Feng et al, 2018](https://doi.org/10.1080/01431161.2018.1479794))

<h4>

(2) How a decision is generated in RF?

    </h4>

We test this forest with the samples that were not used to create the decision trees. These samples are called "*Out of Bag (OOB)*" (30%). The **majority of decisions** made on the OOB are chosen, and the proportion fo OOB incorrectly classified are called OOB error. For example, when we want to classify a pixel, if the majority of decision trees says it is an urban area, the pixel is categorised as an urban area. One thing to note is that OOB is different from test data as test data are never included in building decision trees.

<h3>6.1.3 **Maximum likelihood**</h3>

**Maximum likelihood** is a parametric classifier which expects data to be normally distributed. This classifier uses probability. This means that we can set a probability threshold for a land cover type prior to initialising the classifier. Those pixels whose values are below the set probability are not classified.

<h3>6.1.4 **Support Vector Machine (SVM)**</h3>

<h4>

(1) General Information about SVM

    </h4>

**SVM** is a maximum margin classifier. SVM looks for a place where it can separate datasets most effectively. The benefit of SVM is that it uses structural risk minimisation which minimises errors on unseen data.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/svm.png')
```

Illustration of Support Vector Machine (Source: [skilltohire](https://medium.com/@skilltohire/support-vector-machines-4d28a427ebd))

The distance from the diving line to the closest points is called the maximum margin. However, sometimes points from some datasets are very close to points of other datasets, which could lead to misclassification. This issue can be dealt with *soft margin* that allows some wrongly classified points to get the overall best results. However, as outliers are included within the margins, margins become bigger which could cause **underfitting** problem. Inversely, *hard margin* does not allow any misclassification. This does not allow outliers within the margins so it could cause **overfitting** issue.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/svm_margin.png')
```

Hard and Soft margin (Source: [Velocity Business Solutions](https://www.vebuso.com/2020/02/a-top-machine-learning-algorithm-explained-support-vector-machines-svms/)

<h4>

(2) How to wiggle SVM?

    </h4>

There are 3 hyper-parameters that we can control to wiggle SVM.

-   C: can determine the extent of misclassification that SVM can allow. If 'C' is large, SVM has a hard margin. On the contrary, if 'C' is small, SVM has a soft margin.

-   Kernel: transforms the data when the data cannot be linearly separated.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/svm_kernel.png')
```

Kernel trick which projects data to a higher dimensional space (Source: [Towards Data Science](https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8))

-   gamma: defines the decision boundary. The larger the 'gamma' is, the higher probability of overfitting is. The smaller the 'gamma' is, the more linear the decision boundary is, which might cause underfitting issues.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/svm_gamma.png')
```

Illustration of how change in gamma affects the decision boundary (Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/support-vector-machine-better-understanding/))

The best values for C and gamma can be searched through using grid search, which tests every possible values for the hyper-parameters

<h4>

(3) What is the example of the application of SVM?

    </h4>

Let's say 'pixel 1' has values for band 1, band 2, band 3, and so on. For reference, a pixel which has a number of band values is called **Pattern vector**. If there are forests, they will have many pattern vectors. If we put these into a feature space, these pattern vectors will be very close and on top of each other. This is the moment where SVM can come into play and separate them by allowing some misclassification.

<h2>6.2 Applications</h2>

<h3>6.2.1 General Background</h3>

Some land covers, such as wetland, are hard to classify due to unclear distinction with other surrounding land covers, massive seasonal changes in vegetation and hydrological variation. In this section, I looked into how somewhat classification-wise challenging land covers can be classified in conjunction with ML algorithms.

<h3>6.2.2 Brief Information about a Research</h3>

@rs8110954 aimed to classify wetland land cover by **fusing** the `Pl√©iade-1B data` with `multi-date Landsat-8 data`. The former was performed based on an *object-oriented approach*, and the *geometric* and *spectral features* were extracted. From the latter, the *normalized difference vegetation index (NDVI)* data were calculated and this enabled to reflect phenological changes in vegetation. The feature datasets obtained from two sensors were optimised and used to build a `RF model`.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/wk6_application_abstract.png')
```

Flowchart of building a RF Model (Source: @rs8110954)

The RF classifier obtained an overall accuracy of 93 % and the research team found out that the inclusion of the *geometric shapes* improved the *classification accuracy of the farming lands and water bodies by 5% - 10%*. In addition, often times there was a challenge in classifying wetland due to similar spectral features of vegetation covers. By making use of the *phenological difference* and the *textual information*, the team reduced the classification errors and improved the overall accuracy about 10%.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/wk6_result_contribution.png')
```

Importance of Features in the RF model (Source: @rs8110954)

The classification result of the RF model was compared with other ML classification methods such as Support Vector Machine (SVM) and Artificial Neural Network (ANN). The research team found that the overall accuracy of the RF model was 10% higher than that of the SVM and ANN classifiers.

<h3>6.2.3 Insight from the findings</h3>

The classification results bring us back to a question we had in the lecture. 

*'Does the most advanced ML classification method always achieves the best results?'* 

As shown in the results, the most advanced ML algorithms - SVM and ANN - did not achieve the best classification results. They were 10% behind the overall accuracy compared to the RF model's result. 

The author stated that the way that RF model builds each decision tree was significant in achieving a higher accuracy. In the RF model, each decision tree is a **"specialist tree"** which considers the `feature domain` - the data from geometric and textual features.

For instance, the lakes which are characterised as being large and in a circular shape with a smoother shoreline, which are rather easy to classify. However, marshes and ponds inherit heterogeneous features, and are in irregular shape, which pose difficulties in classifying them. 

Therefore, the **specialist trees** grown with considerations for these features in the RF model explain why the RF model achieved a higher accuracy.
What we can learn from this research is that the location- and context-specific ML algorithm is required to achieve a better classification accuracy. The most advanced and state-of-the-art algorithms do not guarantee the best results.

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/wk6_research_area.png')
```

Research Area (Source: @rs8110954)

```{r echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics('img/wk6_application_results.png')
```

The Classification Results of Different ML Methods - (a) RF; (b) SVM; (c) ANN (Source: @rs8110954)

<h2>6.3 Reflections</h2>

The week 6's lecture covers a number of ML algorithms that are being used in satellite imagery classification. These classification methods are employed to better distinguish one band from the other band which help classifying land cover in the image. They essentially do the same functionality in classifying data but they do it in different ways.

However, there are some aspects that we have to ponder. Firstly, these classifiers often make things very complicated. While state-of-the-art ML algorithms are highly accurate, they are very difficult to interpret.

Secondly, we need to consider why we are using a specific ML algorithm to classify our data. If we could differentiate one band from the other with a simple method, is there still a necessity to use a highly developed classification method?

The last but not the least, we have to think about classification itself. In reality, one pixel is not solely composed of one type of land cover types. It can co-exist with other types of land cover types. Therefore, we need to contemplate on to what extent we have to classify or ignore the values in a pixel.

```{r echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics('img/mixed_pixels.png')
```

Classification of Pixels (Source: [Peter Fisher](https://www.researchgate.net/figure/Four-causes-of-mixed-pixels_fig3_242103275))
