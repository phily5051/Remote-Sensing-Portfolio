[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Phil’s remote sensing learning diary",
    "section": "",
    "text": "Hello, I am Phil from South Korea. This is my learning diary for CASA0023 Remotely Sensing Cities and Environments.\nTo learn more about the module, visit https://andrewmaclachlan.github.io/CASA0023/00-course_info.html"
  },
  {
    "objectID": "week_1.html",
    "href": "week_1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "Summary\n\n\nDefinition of Remote Sensing\n\nAccording to NASA, remote sensing refers to information obtained at a distance. These sensors are placed on satellites or aircrafts and they detect and document reflected or emitted energy.\n\nWhat kinds of energy?\n\nTO cut it short, the answer is Electromagnetic Radiation (EMR). This energy travels in different forms of waves through the atmosphere. While human eyes only detects visible light, the sensors can utilise the full range of the electromagnetic spectrum to collect data.\n\n\n\n\n\n\n\n\n\nElectromagnetic Spectrum (Source: NASA Science)\n\nSensor Types\n\nThere are two types of remote sensors: active and passive sensors.\n\nActive sensor:\n\nemits electromagnetic energy and receives the reflected energy\ncan observe areas under most conditions as most active sensors operate in the microwave band of the electromagnetic spectrum\nrequires power source - solar energy\naffected by space weather - solar flares\n\nPassive sensor:\n\nusually detects reflected energy\nused for measuring physical attributes, such as land/sea surface temperature and vegetation cover\nhas limitations in observing areas in the presence of dense cloud cover\n\n\n\n\n\n\n\n\n\n\n\nPassive and Active Sensors (Source: NASA)\n\nDoes EMR interact with other factors?\n\nYes, these radiations are often influenced by Earth’s surface and atmospheric conditions, which might distort the original information.\n\n\n\n\n\n\n\n\n\nEMR’s interaction with surface and atmosphere (Source: Daneshgar, 2015)\n\n4 Resolutions of Remote Sensing Data\n\n\nSpatial resolution: sizes of the raster cells\nSpectral resolution:\n\nValues for each wavelength across the electromagnetic spectrum creates a spectral signature\nEvery object has its own unique spectral signature, thus it can be used for identifying a specific object\nBut spectral resolution is often affected by atmospheric particles which absorb parts of the spectrum\n\n\n\n\n\n\n\n\n\n\n\nAtmospheric Electromagnetic Opacity (Source: GIS Geography)\n\nTemporal resolution: frequency of the recorded data\nRadiometric resolution: sensor’s ability to detect subtle differences in energy which determines the quality of images\n\n\nApplication\n\nIn the context of climate change, the ability to map tree types are important as they are closely linked to biodiversity. As mentioned earlier, each feature has different spectral signatures. Durgante et al. (2013) have discovered that spectral signature of tree species was better than DNA identification in distinguishing tree species. The researchers used ‘Fourier-Transform Near-Infrared (FT-NIR)’. It obtained the best results of 99.4% of correct specimen identification when using 36 spectral readings per specimen. This somehow opens a new, cost- and time-efficient avenue in mapping forest resources. Their research is meaningful in a sense that it enables us to identify types of forests in which forests are not easily accessible by humans or should be intact from human interferences.\n\n\n\n\n\n\n\n\n\nSpectral signatures of each tree (Source: Durgante et al., 2013)\nThe potential of spectral signature can also be found in the business sector. Yang, Lee, and Williamson (2012) suggested that spectral signatures could be employed to blueberry yield estimation system. The research team confirmed six-class blueberry classification - 233, 551, 554, 691, 699, 1373 nm - yielded the best results. This showed the potential of the spectral signature in developing fast and low cost blueberry detector. Furthermore, this research expands the use of spectral signatures from academia to industry, and suggests that the use of spectral signature can be boundless.\n\n\n\n\n\n\n\n\n\nDifferent absorbance rate depending on blueberry’s growth (Source: Yang and Williamson, 2012)\n\nPersonal Reflection\n\nAs a person who is interested in urban green spaces, the spectral resolution was an interesting concept. After understanding how it can be applied in both academia and industry, I wondered whether the health of green spaces in a city can also be measured with it. Furthermore, I was wondering whether there are any methods that improve the quality of data obtained from passive sensors as they cannot obtain a good quality data in the presence of bad weather. Under the circumstances where we have passive sensors, are there any techniques that could correct images?\n\nReferences\n\nNASA EarthData. (n.d.) Available at: <https://www.earthdata.nasa.gov/> (Accessed: 26.01.2023)\nGIS Geography. (2022) ‘Why the Atmospheric Window Matters in Earth Science’[Online]. Available at: <https://gisgeography.com/atmospheric-window/> (Accessed: 26.01.2023)\n\n\n\n\nDurgante, Flávia Machado, Niro Higuchi, Ana Almeida, and Alberto Vicentini. 2013. “Species Spectral Signature: Discriminating Closely Related Plant Species in the Amazon with Near-Infrared Leaf-Spectroscopy.” Forest Ecology and Management 291: 240–48. https://doi.org/https://doi.org/10.1016/j.foreco.2012.10.045.\n\n\nYang, Ce, Won Suk Lee, and Jeffrey G. Williamson. 2012. “Classification of Blueberry Fruit and Leaves Based on Spectral Signatures.” Biosystems Engineering 113 (4): 351–62. https://doi.org/https://doi.org/10.1016/j.biosystemseng.2012.09.009."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  Week 2",
    "section": "",
    "text": "https://phily5051.github.io/CASA0023_wk2_slides/#1"
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3",
    "section": "",
    "text": "3.1 Summary\n\n\nCauses of Remote Sensing Data Distortion\n\nRemotely sensed images often require corrections to be available for analysis. The image distortions occur due to the following factors:\n\nView angle: depending on the angle of the sensor, the area of interest might look different.\n\n\n\n\n\n\n\n\n\n\nView angle (Source: Shen et al., 2021)\n\nTopography: the shape of terrain might cause some flaws in the image.\n\n\n\n\n\n\n\n\n\n\nThe impact of topography on remote sensing images (Source: Julia Lenhardt)\n\nEarth rotation: Earth’s spinning motion poses another difficulty. Straight lines of an image can appear to be unnaturally curved or deformed.\n\n\n\n\n\n\n\n\n\n\nEffect of Earth’s motion (Source: National Learning Network for Remote Sensing)\n\nWind, and so on.\n\n\nData Corrections\n\n4 types of data correction methods are available for distorted images. These are Geometric, Atmospheric, Orthorectification corrections!\nFirst, we can kick off with Geometric Correction!\n\nGeometric Correction\n\nGeometric errors are normally introduced by internal and external factors (attitude and altitude change). And Ground Control Points (GPS) can be used for correcting the errors! This means, basically, we are identifying so-called “known points” in both sensed image and a reference data and use them to build regression models! But we need each GCP, which are “image coordinate” and “map coordinate”.\n\n\n\n\n\n\n\n\n\nGPS points from a map and Landsat image (Source: GeoLearn)\n\nSolution:\n\nLinear Regression in a “Backward mapping” fashion can be an answer to this problem. Backward mapping is going from output image to input image. This method goes through one pixel at a time and for each position in output image to calculate where in the input image a pixel must come from.\nThe result can be verified by using RMSE. The lower the value is, the more accurate the result is, and the threshold RMSE value is 0.5. To reduce the RMSE, more GPS can be added as well.But when we do this, we have to re-sample the final raster data. There are several resample methods, such as Nearest Neighbour, Linear, Cubic and Cubic spline.\n\n\n\n\n\n\n\n\n\nGeometric Correction (Source: Abdul Basith)\n\n\n\n\n\n\n\n\n\nNearest Neighbour Resampling method (Source: AWF-WIKI)\n\nAtmospheric Correction\n\nAtmospheric correction removes scattering and absorption effects of the atmosphere to improve the image. Absorption and scattering makes an image somewhat blurry as they reduce contrast of image.\n\n\n\n\n\n\n\n\n\nExample of an image before and after atmospheric correction (Source: NASA)\n\nSolution: there are two kinds of approaches - relative and absolute.\n\nRelative atmospheric correction typically consists of two methods.\n- Dark Object Subtraction (DOS): searches each band for the darkest pixel value. The scattering is removed by subtracting this value from every pixel in the band.\n\n\n\n\n\n\n\n\n\nAn improved image after DOS (Source: Wang et al., 2019)\n- Pseudo-Invariant Features (PIFs): is grounded upon statistical invariance of artificial elements, which do not exhibit seasonal variation. The differences in PIFs reflectance between dates are assumed to be due to atmospheric conditions and are linearly related.\n\n\n\n\n\n\n\n\n\nAn improved image after employing PIFs (Source: Schott et al., 1988)\nRelative atmospheric correction convert digital brightness values into scaled surface reflectance by using atmospheric radiative transfer models.\n\n\n\n\n\n\n\n\n\nBefore and after atmospheric correction using radiative transfer method (Source: Advanced Remote Sensing)\n\nOrthorectification Correction\n\nThis method removes distortions by making the images looked right above (at nadir). This correction necessiates sensor geometry and an elevation model.\n\n\n\n\n\n\n\n\n\nAn example of orthorectification correction (Source: Esri Insider, 2016)\n\nData Join\n\nJoining remote sensed images are coined as “Mosaicking”. Using histogram matching algorithm, similar brightness values are given to two images that are overlapped, and then the feathering is conducted.\n\n\n\n\n\n\n\n\n\nSix raster datasets are joined into one image (Source: ArcMap)\n\nImage Enhancement\n\nMaterials reflect different amounts of energy in the same wavelengths. However, sensors have a fixed range of Digital Number (DN) values between 4 to 105. If we expand this range, we could enhance the visual appearance of images. There are several methods to do this, which include Minimum-Maximum, Percentage Linear and Standard Deviation, and Piecewise Linear Contrast Stretch.\n\nMinimum-Maximum: utilises the full range of available brightness values\n\n\n\n\n\n\n\n\n\n\nMinimum-Maximum Linear Stretch (Source: OneStopGIS)\n\nPercentage Linear and Standard Deviation Stretch: is similar to the Minimum-Maximum linear contrast stretch but this method uses a specified minimum and maximum values that situate between a certain percentage of pixels from the mean of the histogram.\n\n\n\n\n\n\n\n\n\n\nPercentage Linear Contrast Stretch (Source: OneStopGIS)\n\nPiecewise Linear Contrast Stretch: is used when the distribution of a histogram in an image is bi or trimodal (Jensen and Schill, n.d.).\n\n\n\n\n\n\n\n\n\n\nPiecewise Linear Contrast Stretch (Source: Tsai and Yeh, 2008)\n\n3.2 Application\n\nImage enhancement techniques not only enhance visual appearance but also allows for comparison between images retrieved from different years.\nHan-qiu (2011) has attempted to compare thermal images from different years with the same date in order to investigate urban heat island (UHI). The thermal infrared bands of different date were processed with several image enhancement technologies.\nTo detect change of the UHI between different dates, the two thermal imageries were normalised and scaled to reduce the impacts caused by seasonal variations or weather conditions. Then, the images were overlaid to show difference between images by subtracting correponding pixels.\nThis research demonstrates how we can employ image enhancement technologies to compare images from different years in a quantitative fashion.\nIn addition to image enhancement technologies covered in the lecture, Hu et al. (2020) demonstrated that Convolutional Neural Networks (CNN) can enhance visual appearance of image greatly. The authors carried out performing several different image enhancement technologies on low-quality image.\n\n\n\n\n\n\n\n\n\nResults of different image enhancement methods (Source: Hu et al., 2020)\nTheir findings suggest remote-sensing CNN (RSCNN) has proven to be the most efficient method in enhancing visualisation effects and allowing a better interpretation. RSCNN showed the superiority in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise_Ratio (PSNR) over conventional techniques.\n\n\n\n\n\n\n\n\n\nSSIM and PSNR of different image enhancement methods (Source: Hu et al., 2020)\nThis research somehow offered potentials in combining two different fields of studies - CNN and remote sensing -, and suggested that adding complexity to imagery can help us more natural results with more realistic textures and vivid details.\n\n3.3 Reflection\n\nThis week’s content gave me an answer for the questions I had from the first week. Numerous data correction technologies improve the quality of image collected and allow for facilitating data to be ready for analysis.\nHowever, as there are a variety of correction methods, it seems necessary to understand what causes flaws in the remotely sensed images and how to employ an appropriate correction method to that specific datasets. In addition, I wonder whether there is a preference in a particular correction method over other correction methods, depending on the domain of study field or industry.\nLastly, as we went through numerous technologies which enable us to identify objects or areas in a better and higher resolution. However, in the context of built environment, there are many objects in cities that look similar to each other, and this might pose a difficulty in classifying them.\nIn Data Science module, I came across a deep learning model called YOLOv5, which detects objects in remotely sensed images. The model was trained on custom datasets. Once the training is done, the model can be applied to satellite imagery of different resolutions and identify objects.\n\n\n\n\n\n\n\n\n\nObjects indicated by a bounding box and its probability (Source: Google Earth Engine for OSINT)\nIf we create a customised dataset which contains specific information about some types of urban infrastructure, and feed these to train a model, we might be able to build a deep learning object detection model targeted at urban environment. Together with the data correction and image enhancement methods we learnt, I think the object detection model could enable us to classify some technically-challenging objects in an urban setting.\n\n3.4 References\n\nGoogle Earth Engine for OSINT. (n.d.) ‘Object Detection in Satellite Imagery’[Online]. Available at: https://oballinger.github.io/GEE_OSINT/object_detection.html (Acessed: 08.02.2023)\nJensen, J.H. and Schill, S.R. (n.d.) ‘Contrast Enhancement’[PDF]. Available at: http://knightlab.org/rscc/legacy/RSCC_Contrast_Enhancement.pdf (Accessed: 08.02.2023)\n\n\n\n\nHan-qiu, Chen. 2011. “An Image Processing Technique for the Study of Urban Heat Island Changes Using Different Seasonal Remote Sensing Data.” Remote Sensing Technology and Application 18: 129–33.\n\n\nHu, Linshu, Mengjiao Qin, Feng Zhang, Zhenhong Du, and Renyi Liu. 2020. “RSCNN: A CNN-Based Method to Enhance Low-Light Remote-Sensing Images.” Remote Sensing 13 (December): 62. https://doi.org/10.3390/rs13010062."
  },
  {
    "objectID": "week_4.html",
    "href": "week_4.html",
    "title": "4  Week 4",
    "section": "",
    "text": "Summary\n\nIn this diary, I am going to talk about the biggest city plan in South Korea and how a low-level city plan guideline is aligned with its upper-level plan. In particular, I will focus on the implementation of the plan regarding environment and whether there is any implementation that can be benefited from using remote sensing.\n\nBrief Information about 2030 Seoul Plan\n\n2030 Seoul Plan is the highest level city plan in Seoul, South Korea. Provided that Seoul is a capital city in South Korea, its city plan is often considered as significant and determinant as a government-led plan, in terms of its impacts on citizens of Seoul and the country. The plan covers a number of sectors which include housing, transport and environment. The city plan presents Seoul’s future vision and outlines the city’s development plan for the next 10 years.\nWith regards to environment, the plan specifies 3 Targets and 11 Strategies. The 3 targets are as follows:\n\nTarget 1: Ecological city with numerous urban parks\nTarget 2: Energy-efficient city\nTarget 3: Disaster-free city\n\nAmong the illustrated targets, Strategy 1-4 under Target 1 suggests a number of ways to deal with particulate matter, which is one of the biggest environmental issues in South Korea and highly related to citizen’s life. One of the mitigation actions to deal with the air pollutants identified by the Seoul government is creating urban forests on those areas which are specifically vulnerable to fine dust.\n\n\n\n\n\n\n\n\n\n2030 Seoul Plan (Source: Seoul Metropolitan Government)\n\nUrban Forest Guideline for PM Mitigation\n\nUrban Forest Guideline for PM Mitigation is a low-level city plan which entails types of activities that will be executed in line with the 2030 Seoul Plan. The guideline specified a number of technical rules to follow regarding types of trees to plant, which areas are targeted for urban forests implementation, and suitable distance between each tree stand and so on.\nFurthermore, the guideline categorised six types of urban forests that could mitigate the concentration of PM. These are street trees, urban parks, forests around the city, vegetations in schools and river, and green spaces within proximity to residential areas.\n\n\n\n\n\n\n\n\n\nDistance between trees in residential areas (Source: Seoul Metropolitan Government)\n\n3 Functional Urban Forest Types for PM Mitigation\n\nThe said urban forest types can be grouped into 3 types of urban forests in terms of their objective and functionality. These are PM blockage urban forests, PM mitigation urban forests and Wind corridor urban forests.\n\nPM blockage urban forests:\n\ndense urban forests with 1,800 trees per ha\nto block PM diffusion\n\nPM mitigation urban forests:\n\nbetween 800 and 1,000 trees per ha\nto enhance PM absorption by urban forests\n\nWind corridor urban forests:\n\n500 trees per ha\nto introduce clean air to city centre\nto enable the outflow of congregated PM within the city due to vehicles and household heating.\n\n\n\n\n\n\n\n\n\n\n\n3 Types of PM Mitigation Urban Forest Types (Adjusted from Korea Forest Service)\n\nApplication\n\nRemote sensing can come into play a key role in identifying what PM mitigation urban forest types are most suitable to a particular site within a city, and quantifying the amounts of PM absorbed by urban forests.\nRemote sensing is an effective measure to map and monitor PM concentrations (Zhang et al. 2021). This indicates that areas with routinely high PM concentrations can be detected by satellite images. Thus, government officials could take data-informed decisions and select the most suitable PM mitigation urban forest types for each particular area.\n\n\n\n\n\n\n\n\n\nAverage PM2.5 concentration between 2001 and 2006 mapped by remote sensing (Source: Zhang et al. (2021))\nIn addition, Gupta et al. (2006) found that remote sensing can capture the movement of air pollutants over a large area as satellite measurements are available globally. Their findings allude to us that remote sensing could enable us to understand how much PM have been absorbed by urban forests by measuring the size of PM mass between areas. The policymakers could draw on these quantified data as a basis for future policies.\n\n\n\n\n\n\n\n\n\nDocumentation of 1-year change of PM2.5 mass concentration in five study regions (Source: Gupta et al. (2006))\n\nLink to UN SDG Goals\n\nAccording to United Nations 2030 Sustainable Development Goals (SDGs), the use of remote sensing could contribute to achieving 2 SDGs goals.\n\nSDG 3. Good Health and Well-Being\n\nPM causes various health problems, such as lung and heart diseases. The reduction in PM could improve citizen’s health and quality of life.\n\nSDG 11. Sustainable Cities and Communities\n\nCreating urban forests, as a measure to mitigate PM, could not only reduce aereal PM but also contribute to building a safe and resilient built environment from other natural hazards.\n\n\n\nReflection\n\nThis week’s contents were somewhat different from what we have learnt past few months. Understanding policy and how remote sensing could come into play its part in realising the policy’s objective was interesting. The contemplation upon how remote sensing can be applied at a particular context and a site broadened my understanding. Of course, I am aware that remote sensing is one of many solutions that could assist in achieving policy’s goals, and 1-size-fits-all approach is not sufficient to solve similar problems in different contexts. Thanks to this week’s lecture, however, I could say that I came to understand how we can apply our theoretical and technical knowledge about remote sensing to solve the real problems.\n\nReferences\n\nKorea Forest Service. (n.d.) https://english.forest.go.kr/kfsweb/kfs/subIdx/Index.do?mn=UENG (Accessed: 02.02.2022)\nSeoul Metropolitan Government. (n.d.) https://www.seoul.go.kr/main/index.jsp (Accessed: 02.02.2022)\nUnited Nations. (n.d.) https://sdgs.un.org/goals (Accessed: 02.02.2022)\n\n\n\n\nGupta, Pawan, Sundar A. Christopher, Jun Wang, Robert Gehrig, Yc Lee, and Naresh Kumar. 2006. “Satellite Remote Sensing of Particulate Matter and Air Quality Assessment over Global Cities.” Atmospheric Environment 40 (30): 5880–92. https://doi.org/https://doi.org/10.1016/j.atmosenv.2006.03.016.\n\n\nZhang, Ying, Zhengqiang Li, Kaixu Bai, Yuanyuan Wei, Yisong Xie, Yuanxun Zhang, Yang Ou, et al. 2021. “Satellite Remote Sensing of Atmospheric Particulate Matter Mass Concentration: Advances, Challenges, and Perspectives.” Fundamental Research 1 (3): 240–58. https://doi.org/https://doi.org/10.1016/j.fmre.2021.04.007."
  },
  {
    "objectID": "week_5.html",
    "href": "week_5.html",
    "title": "5  Week 5",
    "section": "",
    "text": "5.1 Summary\n\n\n5.1.1 What is Google Earth Engine (GEE)\n\nGoogle Earth Engine is a planetary-scale geo-spatial analysis platform. It enables users to keep track of changes and quantify differences on Earth’s surface.\n\n\n\n\n\n\n\n\n\nIllustration of GEE Mechanism (Source: Google Earth Engine)\n\n5.1.2 Pros & Cons of GEE\n\nThe advantages and disadvantages of GEE are as follows:\n\nPros:\n\nGEE stores various, rich and ready-to-use datasets within its server.\nCloud-based processing: quickly analyses big data\nUser-friendly interface: easy and free access\nEnormous potential to collaborate with state-of-the-art technologies (Deep learning and Machine Learning)\n\nCons:\n\nEasy access could mean the potential possibilities for inappropriate use by some criminals\nDependence on Google: concerns around data privacy and security\nLearning curve: coding with Javascript can be challenging\nLimited data types: profoundly limited to satellite imagery, which may not provide access to all the data types that people need\n\n\n\n\n\n\n\n\n\n\n\nGEE Data Catalog (Source: Google Earth Engine)\n\n5.1.3 Aggregating Pixels in GEE\n\nTo allow large computations, GEE provides users with various scale options to choose from. When an image is fed into GEE, many lower resolution versions of the image are pre-computed, and these are known as Image Pyramids.\n\n\n\n\n\n\n\n\n\nGEE Image Pyramids (Source: Google Earth Engine)\nThe lowest level of the image pyramid represents native resolution. The ingested image data are aggregated to a higher pyramid levels until it reaches 256 * 256 pixel tiles. At this aggregation process, GEE uses nearest neighbors by default Google Earth Engine. By default, the pyramid tiles are created by calculating mean values. This is called resampling.\n\n\n\n\n\n\n\n\n\nExample of resampling (Source: SpatialThoughts)\n\n5.1.4 Objects in GEE\n\nThere are 9 object classes in GEE. Each class has its own class-specific functions to load and manipulate data.\n\n\n\n\n\n\n\n\n\nEarth Engine Class Types (Source: Google Earth Engine)\n\n5.1.5 Applicable Processes in GEE\n\n\nReducing images by regions:\n\nBy region(s): It reduces all the pixels in the region(s) to a statistic of the pixel data in the region(s). We can take an image and generate statistics for it. The images of code and result below are adjusted from Andy’s Material. I used the Global Forest Change datasets to see the average reflectance for each band within Sierra Nevada, USA.\n\n\n\n\n\n\n\n\n\n\n\nCode for reducing images by region - Mean values of Tree loss & Tree cover\n\n\n\n\n\n\n\n\n\nResults of reducing images by region on GEE Code Editor - Mean values of Tree loss & Tree cover\n\nReducing images by neighbourhoods:\n\nBy neighbourhoods: The neighbourhoods of a pixel in an image can be used to reduce the image.\n\n\n\n\n\n\n\n\n\n\n\nReducing Images by neighbourhoods (Google Earth Engine)\n\n5.2 Application\n\nGoogle Earth Engine has been widely applied, ranging from forest and vegetation studies to medical fields such as malaria (Kumar and Mutanga 2018). The display of satellite imagery on GEE has enabled us to identify any change occurred during a certain period of time. However, I was wondering whether detecting changes in land cover is the only thing that GEE can offer. In this section, I will focus on how GEE can be applied in collaboration with Machine Learning technologies, and what are the benefits of considering technological fusion.\nGEE can be an effective way of monitoring and mapping land cover. Brovelli, Sun, and Yordanov (2020) mapped and monitored the rainforest change in Brazil from 2000 and 2019. The forest cover was mapped at a 5-year period by using a Machine Learning algorithm on GEE platform.\n\n\n\n\n\n\n\n\n\nWork Flow of the research (Source: Brovelli, Sun, and Yordanov (2020))\nThe Random Forest (RF) was used to classify past and present land cover - forest and non-forest. The whole process of building the model was conducted on GEE platform. 80% of the datasets were used for training the model, and the rest was withheld for testing the accuracy of the model. Then, the output classification results were validated through high-resolution satellite images.\n\n\n\n\n\n\n\n\n\nRF Classification results (Source: Brovelli, Sun, and Yordanov (2020))\n\n\n\n\n\n\n\n\n\nValidation samples for RF model (Source: Brovelli, Sun, and Yordanov (2020))\nArtificial Neural Network (ANN) was used to estimate the future state of the forests - the future forest cover.\n\n\n\n\n\n\n\n\n\nANN simulation result between 2010 and 2014 (Source: Brovelli, Sun, and Yordanov (2020))\n\n\n\n\n\n\n\n\n\nANN simulation result between 2019 and 2028 (Source: Brovelli, Sun, and Yordanov (2020))\nThe above research was utilising GEE as a main platform to build a ML model. By building a model, it allowed us for predicting the future state of the forests. The research also elaborated how they tested the RF model’s performance. However, if there is an issue of over-fitting in a model, is there any way that we could deal with this issue?\nFurthermore, the authors did not explain how they validated the ANN model. While the simulation results could provide us to predict the future state of the forests, there was no dataset or method that tested the model’s performance which casts a doubt on the ANN model.\n\n5.3 Reflection\n\nThis week’s content was an introductory lecture for the GEE. As I have not heard of GEE before, the whole concept was somewhat very new and interesting for me.\nThe use of Javascript to write code looked similar to Python, but it was quite different in terms of defining variables. In particular, the fact that datasets are stored within GEE server was very convenient. Storing data always took up a lot of memory in my computer and sometimes it was hard to set up or remember a directory whenever I had to proceess data on R. The datasets on server saved so much space in my computer as well as the ready-made code to import the datasets was really easy.\nIn addition, spatial join and reducing images were much faster on GEE. With a few lines of code, I was able to get the results I wanted which would require lots of lines in other programming languages.\nLastly, after doing a research on the application of GEE, I noticed that most of datasets stored within the GEE server were mostly about environment-related datasets, and many of the on-going research were mainly focusing on identifying change in the environment. Thus, I was wondering whether there is any current research in the context of urban environment. The change in an urban setting might not be as distinctive as deforestation or change in temperature. However, cities are growing and urban infrastructures are always changing. This gives us enough reason to make use of GEE to better understand fast-changing cities. Therefore, in the next session, I would love to do more research about how GEE can be potentially utilised in the urban environment.\n\nReferences\n\nGoogle Earth Engine. (n.d.) https://earthengine.google.com/ (Accessed: 28.02.2023)\nSpatialThoughts. (2021) https://spatialthoughts.com/2021/05/13/aggregating-population-data-gee/ (Accessed: 28.02.2023)\n\n\n\n\nBrovelli, Maria Antonia, Yaru Sun, and Vasil Yordanov. 2020. “Monitoring Forest Change in the Amazon Using Multi-Temporal Remote Sensing Data and Machine Learning Classification on Google Earth Engine.” ISPRS International Journal of Geo-Information 9 (10). https://doi.org/10.3390/ijgi9100580.\n\n\nKumar, Lalit, and Onisimo Mutanga. 2018. “Google Earth Engine Applications Since Inception: Usage, Trends, and Potential.” Remote Sensing 10 (10). https://doi.org/10.3390/rs10101509."
  },
  {
    "objectID": "week_6.html",
    "href": "week_6.html",
    "title": "6  Week 6",
    "section": "",
    "text": "6.1 Summary\n\nIn this summary, I will go through several Machine Learning (ML) classification methods. The classification methods can be divided into two approach - unsupervised and supervised algorithms. However, this diary will mainly focus on the supervised algorithms.\nSupervised ML follows the following process:\n- Class definition\n- Pre-processing\n- Training\n- Pixel assignment\n- Accuracy assessment\n\n\n\n\n\n\n\n\n\nSupervised Classification Process in Remote Sensing (Source: GISgeography)\n\n6.1.1 Classification and Regression Trees (CART)\n\n\n\nGeneral Information about CART\n\n\nCART is a binary split and it consists of classification trees and regression trees. My interpretation of CART is that when we put our data on a 2D graph, we can see that data points cannot be solely captured by a single straight line.\n\n\n\n\n\n\n\n\n\nCART Algorithm (Source: DataDrivenInvestor)\nThus, we subset our data into smaller groups and calculate the average for each data chunk. Each chunk is composed of some nodes. We add up nodes in the chunk one by one and check to what extent adding nodes gives the lowest sum of squared residuals (SSR). To a certain point, adding nodes to the chunk would return a lower SSR, however, at some point we will notice that adding an extra node would give us a higher SSR. In this manner, We check SSR for different threshold till we get the lowest SSR. When we identify the lowest SSR, the average of the nodes in that chunk becomes a threshold. This threshold is where a decision tree divides. This indicates if we subset our data in many times, we can get a precise and accurate model. However, this poses a problem of over-fitting.\n\n\n\n\n\n\n\n\n\nCART SSR Threshold (Source: DataDrivenInvestor)\n\n\nHow to prevent over-fitting in CART?\n\n\nThere are two methods to prevent over-fitting in CART.\n\nSetting Maximum Tree Depth: can limit how much trees can grow. 20 is default.\n\n\n\n\n\n\n\n\n\n\nCART with Maximum Tree Depth (Source: Towards Data Science)\n\nWeakest link pruning: basically means that we use a full size regression tree and we see if removing a leaf (a chunk of nodes) gives a lower tree score. We start with 0 for the value of \\(\\alpha\\), and increase it until we get a lower tree score. The equation for tree score is as below:\n\n\\(Tree-score\\) = \\(SSR\\) + \\(\\alpha\\) * \\(T(Number-of-leaves)\\)\nOnce we have those values for \\(\\alpha\\), we divide our data into training and test data. We take training data with all the values for \\(\\alpha\\) and calculate the SSR. Then, we look for which \\(\\alpha\\) value returns the lowest SSR. We repeat this process several times and look for the average value until we get the lowest SSR.\n\n\n\n\n\n\n\n\n\nCART with Pruning (Source: Towards Data Science)\n\n6.1.2 Random Forests (RF)\n\n\n\nGeneral Information about RF\n\n\nRF simply means that many are better than one. We do “bootstrap” samples (bagging - only 70% of data is used), and create nodes with random number of variables and on and on. Eventually, we will make many decision trees from random number of variables. These different decision trees are called a forest.\n\n\n\n\n\n\n\n\n\nRandom Forests Classification (Source: Feng et al, 2018)\n\n\nHow a decision is generated in RF?\n\n\nWe test this forest with the samples that were not used to create the decision trees. These samples are called “Out of Bag (OOB)” (30%). The majority of decisions made on the OOB are chosen, and the proportion fo OOB incorrectly classified are called OOB error. For example, when we want to classify a pixel, if the majority of decision trees says it is an urban area, the pixel is categorised as an urban area. One thing to note is that OOB is different from test data as test data are never included in building decision trees.\n\n6.1.3 Maximum likelihood\n\nMaximum likelihood is a parametric classifier which expects data to be normally distributed. This classifier uses probability. This means that we can set a probability threshold for a land cover type prior to initialising the classifier. Those pixels whose values are below the set probability are not classified.\n\n6.1.4 Support Vector Machine (SVM)\n\n\n\nGeneral Information about SVM\n\n\nSVM is a maximum margin classifier. SVM looks for a place where it can separate datasets most effectively. The benefit of SVM is that it uses structural risk minimisation which minimises errors on unseen data.\n\n\n\n\n\n\n\n\n\nIllustration of Support Vector Machine (Source: skilltohire)\nThe distance from the diving line to the closest points is called the maximum margin. However, sometimes points from some datasets are very close to points of other datasets, which could lead to misclassification. This issue can be dealt with soft margin that allows some wrongly classified points to get the overall best results. However, as outliers are included within the margins, margins become bigger which could cause underfitting problem. Inversely, hard margin does not allow any misclassification. This does not allow outliers within the margins so it could cause overfitting issue.\n\n\n\n\n\n\n\n\n\nHard and Soft margin (Source: Velocity Business Solutions\n\n\nHow to wiggle SVM?\n\n\nThere are 3 hyper-parameters that we can control to wiggle SVM.\n\nC: can determine the extent of misclassification that SVM can allow. If ‘C’ is large, SVM has a hard margin. On the contrary, if ‘C’ is small, SVM has a soft margin.\nKernel: transforms the data when the data cannot be linearly separated.\n\n\n\n\n\n\n\n\n\n\nKernel trick which projects data to a higher dimensional space (Source: Towards Data Science)\n\ngamma: defines the decision boundary. The larger the ‘gamma’ is, the higher probability of overfitting is. The smaller the ‘gamma’ is, the more linear the decision boundary is, which might cause underfitting issues.\n\n\n\n\n\n\n\n\n\n\nIllustration of how change in gamma affects the decision boundary (Source: Analytics Vidhya)\nThe best values for C and gamma can be searched through using grid search, which tests every possible values for the hyper-parameters\n\n\nWhat is the example of the application of SVM?\n\n\nLet’s say ‘pixel 1’ has values for band 1, band 2, band 3, and so on. For reference, a pixel which has a number of band values is called Pattern vector. If there are forests, they will have many pattern vectors. If we put these into a feature space, these pattern vectors will be very close and on top of each other. This is the moment where SVM can come into play and separate them by allowing some misclassification.\n\n6.2 Applications\n\n\n6.2.1 General Background\n\nSome land covers, such as wetland, are hard to classify due to unclear distinction with other surrounding land covers, massive seasonal changes in vegetation and hydrological variation. In this section, I looked into how somewhat classification-wise challenging land covers can be classified in conjunction with ML algorithms.\n\n6.2.2 Brief Information about a Research\n\nTian et al. (2016) aimed to classify wetland land cover by fusing the Pléiade-1B data with multi-date Landsat-8 data. The former was performed based on an object-oriented approach, and the geometric and spectral features were extracted. From the latter, the normalized difference vegetation index (NDVI) data were calculated and this enabled to reflect phenological changes in vegetation. The feature datasets obtained from two sensors were optimised and used to build a RF model.\n\n\n\n\n\n\n\n\n\nFlowchart of building a RF Model (Source: Tian et al. (2016))\nThe RF classifier obtained an overall accuracy of 93 % and the research team found out that the inclusion of the geometric shapes improved the classification accuracy of the farming lands and water bodies by 5% - 10%. In addition, often times there was a challenge in classifying wetland due to similar spectral features of vegetation covers. By making use of the phenological difference and the textual information, the team reduced the classification errors and improved the overall accuracy about 10%.\n\n\n\n\n\n\n\n\n\nImportance of Features in the RF model (Source: Tian et al. (2016))\nThe classification result of the RF model was compared with other ML classification methods such as Support Vector Machine (SVM) and Artificial Neural Network (ANN). The research team found that the overall accuracy of the RF model was 10% higher than that of the SVM and ANN classifiers.\n\n6.2.3 Insight from the findings\n\nThe classification results bring us back to a question we had in the lecture.\n‘Does the most advanced ML classification method always achieves the best results?’\nAs shown in the results, the most advanced ML algorithms - SVM and ANN - did not achieve the best classification results. They were 10% behind the overall accuracy compared to the RF model’s result.\nThe author stated that the way that RF model builds each decision tree was significant in achieving a higher accuracy. In the RF model, each decision tree is a “specialist tree” which considers the feature domain - the data from geometric and textual features.\nFor instance, the lakes which are characterised as being large and in a circular shape with a smoother shoreline, which are rather easy to classify. However, marshes and ponds inherit heterogeneous features, and are in irregular shape, which pose difficulties in classifying them.\nTherefore, the specialist trees grown with considerations for these features in the RF model explain why the RF model achieved a higher accuracy. What we can learn from this research is that the location- and context-specific ML algorithm is required to achieve a better classification accuracy. The most advanced and state-of-the-art algorithms do not guarantee the best results.\n\n\n\n\n\n\n\n\n\nResearch Area (Source: Tian et al. (2016))\n\n\n\n\n\n\n\n\n\nThe Classification Results of Different ML Methods - (a) RF; (b) SVM; (c) ANN (Source: Tian et al. (2016))\n\n6.3 Reflections\n\nThe week 6’s lecture covers a number of ML algorithms that are being used in satellite imagery classification. These classification methods are employed to better distinguish one band from the other band which help classifying land cover in the image. They essentially do the same functionality in classifying data but they do it in different ways.\nHowever, there are some aspects that we have to ponder. Firstly, these classifiers often make things very complicated. While state-of-the-art ML algorithms are highly accurate, they are very difficult to interpret.\nSecondly, we need to consider why we are using a specific ML algorithm to classify our data. If we could differentiate one band from the other with a simple method, is there still a necessity to use a highly developed classification method?\nThe last but not the least, we have to think about classification itself. In reality, one pixel is not solely composed of one type of land cover types. It can co-exist with other types of land cover types. Therefore, we need to contemplate on to what extent we have to classify or ignore the values in a pixel.\n\n\n\n\n\n\n\n\n\nClassification of Pixels (Source: Peter Fisher)\n\n\n\n\nTian, Shaohong, Xianfeng Zhang, Jie Tian, and Quan Sun. 2016. “Random Forest Classification of Wetland Landcovers from Multi-Sensor Data in the Arid Region of Xinjiang, China.” Remote Sensing 8 (11). https://doi.org/10.3390/rs8110954."
  },
  {
    "objectID": "week_8.html",
    "href": "week_8.html",
    "title": "7  Week 8",
    "section": "",
    "text": "In this diary, I am going to give general information about Urban Heat Island (UHI) with some on-going UHI mitigation examples all around the world.\nThen, I will explain how UHI-related policies at a global-, metropolitan-, and local-level vary in terms of its goal and implementation scale, and ponder how we could bridge the gap that exists between these different levels of policies.\n\n8.1 Summary\n\n\nWhat is Urban Heat Island?\n\n\nDefinition: UHI is a phenomenon where the temperature in an urban area is much warmer than its rural areas.\n\n\n\n\n\n\nDifference in Temperature due to Urban Heat Island (Source: US EPA, 2022)\n\n\n\n\n\nCauses:\n\nUrban materials which reflect less solar energy and absorb more sun’s heat - building roofs, pavements and parking lots (US EPA, 2022)\nLack of urban green spaces which do evapo-transpiration and solar blocking (US EPA, 2022)\nHeat generating human activities, such as A/C, industrial activities and vehicles, etc) (US EPA, 2022)\nUrban geometry which blocks wind corridor - tall buildings and narrow streets create urban canyons (US EPA, 2022)\n\n\n\n\n\n\n\nUrban Heat Island Effects and Factors (Source: World Bank Blogs, 2020)\n\n\n\n\n\nImpacts:\n\nSocial impacts:\n\nIncreased energy consumption, which cause a positive feedback loop\nHeat-related illness and increased mortality rate due to heat stroke\n\nEnvironmental impacts:\n\nHigher temperature during the daytime and reduced nighttime cooling effects\nIncreased green house gas emissions\n\nEconomic impacts:\n\nIncreased price of electricity\nRise in food price due to dry and hot weather\n\n\n\n\n8.2 Application\n\nIn this section, I am going to introduce Seoul’s urban planning policy with respect to UHI, and see how UHI mitigation activities under the metropolitan policy is in line with global policy.\nSeoul Plan is a metropolitan-level urban planning policy in Seoul, South Korea. The plan consists of 5 focal points, and one of which is environment. Under the environment part, there are 3 targets and 11 strategies. The strategy 1-2: Enhancement of Urban Climate Control Capacity specifies UHI. The strategy lists a number of UHI reduction activities which are as follows:\n\nEnabling wind corridor which brings fresh air from forests to city centre\n\n\n\n\n\n\nWind corridor forest map. (Source: Seoul Metropolitan Government)\n\n\n\n\nNote. Arrows indicate the main wind flow route. Forests play a key role in directing wind flow to city centre.\n\nImplementation of mist spray and shade canopy in the parks and bus stops\n\n\n\n\n\n\nMist spray and shade canopy (Source: The JoongAng, 2019)\n\n\n\n\n\n\n\nMist spray and shade canopy (Source: The JoongAng, 2019)\n\n\n\n\n\nConfiguration of urban waterfronts and urban parks\n\n\n\n\n\n\nWaterfront and urban parks in Seoul (Source: left image by Seoul Metropolitan Government & right image by Kookmin Ilbo)\n\n\n\n\n\n\n\nWaterfront and urban parks in Seoul (Source: left image by Seoul Metropolitan Government & right image by Kookmin Ilbo)\n\n\n\n\nSeoul Plan’s UHI reduction activities are in accordance with UN SDG 11. The activities listed contribute to building an environment risk resilient city. In particular, increasing open and green spaces within an urban setting is related to UN SDG 11.7.1: Average share of the built-up area of cities that is open space for public use for all, by sex, age and persons with disabilities.\n\n\n\n\n\nUN SDG 11 - Global Policy (Source: United Nations)\n\n\n\n\nHowever, some of the mitigation activities do not necessarily cope with main causes of UHI. There should also consideration for road paving materials which enable percolation of rainwater. In addition, more specific and granular-scale guidelines for implementing UHI-mitigation urban parks are needed to better locate the urban parks within the city. This is where remote sensing data could come into play in deciding the optimal location of urban green spaces. Sentinel 3 can be used to detect highly unusual temperature in cities.\n\n8.3 Reflection\n\nThis week’s lecture was an extended lecture of the week 4, where we learnt about policies. A few weeks ago, I did not really understand the reason why there exist varying levels of policies. As each country faces country-specific challenges and their contexts are quite different. Therefore, I had somewhat suspicion about following global policies.\nHaving UHI as an example enabled me to grasp the importance of creating a cohesive link between different levels of policies. It is true that global policies are ambiguous and provide unclear guidance, and each country has different context. Yet, we need to have an agreed, standardised approach as it helps us assess and monitor the actions we take, and allows for comparing outputs.\n\nReferences\n\nUnited States Environmental Protection Agency (US EPA). (2022) https://www.epa.gov/heatislands/learn-about-heat-islands (Accessed: 14.03.2023)\nWorld Bank Blogs (2020) https://blogs.worldbank.org/sustainablecities/3-ways-beat-heat-european-cities (Accessed: 14.03.2023)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brovelli, Maria Antonia, Yaru Sun, and Vasil Yordanov. 2020.\n“Monitoring Forest Change in the Amazon Using Multi-Temporal\nRemote Sensing Data and Machine Learning Classification on Google Earth\nEngine.” ISPRS International Journal of Geo-Information\n9 (10). https://doi.org/10.3390/ijgi9100580.\n\n\nDurgante, Flávia Machado, Niro Higuchi, Ana Almeida, and Alberto\nVicentini. 2013. “Species Spectral Signature: Discriminating\nClosely Related Plant Species in the Amazon with Near-Infrared\nLeaf-Spectroscopy.” Forest Ecology and Management 291:\n240–48. https://doi.org/https://doi.org/10.1016/j.foreco.2012.10.045.\n\n\nGupta, Pawan, Sundar A. Christopher, Jun Wang, Robert Gehrig, Yc Lee,\nand Naresh Kumar. 2006. “Satellite Remote Sensing of Particulate\nMatter and Air Quality Assessment over Global Cities.”\nAtmospheric Environment 40 (30): 5880–92. https://doi.org/https://doi.org/10.1016/j.atmosenv.2006.03.016.\n\n\nHan-qiu, Chen. 2011. “An Image Processing Technique for the Study\nof Urban Heat Island Changes Using Different Seasonal Remote Sensing\nData.” Remote Sensing Technology and Application 18:\n129–33.\n\n\nHu, Linshu, Mengjiao Qin, Feng Zhang, Zhenhong Du, and Renyi Liu. 2020.\n“RSCNN: A CNN-Based Method to Enhance Low-Light Remote-Sensing\nImages.” Remote Sensing 13 (December): 62. https://doi.org/10.3390/rs13010062.\n\n\nKumar, Lalit, and Onisimo Mutanga. 2018. “Google Earth Engine\nApplications Since Inception: Usage, Trends, and Potential.”\nRemote Sensing 10 (10). https://doi.org/10.3390/rs10101509.\n\n\nTian, Shaohong, Xianfeng Zhang, Jie Tian, and Quan Sun. 2016.\n“Random Forest Classification of Wetland Landcovers from\nMulti-Sensor Data in the Arid Region of Xinjiang, China.”\nRemote Sensing 8 (11). https://doi.org/10.3390/rs8110954.\n\n\nYang, Ce, Won Suk Lee, and Jeffrey G. Williamson. 2012.\n“Classification of Blueberry Fruit and Leaves Based on Spectral\nSignatures.” Biosystems Engineering 113 (4): 351–62.\nhttps://doi.org/https://doi.org/10.1016/j.biosystemseng.2012.09.009.\n\n\nZhang, Ying, Zhengqiang Li, Kaixu Bai, Yuanyuan Wei, Yisong Xie, Yuanxun\nZhang, Yang Ou, et al. 2021. “Satellite Remote Sensing of\nAtmospheric Particulate Matter Mass Concentration: Advances, Challenges,\nand Perspectives.” Fundamental Research 1 (3): 240–58.\nhttps://doi.org/https://doi.org/10.1016/j.fmre.2021.04.007."
  }
]