[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Phil’s remote sensing learning diary",
    "section": "",
    "text": "Hello, I am Phil from South Korea. This is my learning diary for CASA0023 Remotely Sensing Cities and Environments.\nTo learn more about the module, visit https://andrewmaclachlan.github.io/CASA0023/00-course_info.html"
  },
  {
    "objectID": "week_1.html",
    "href": "week_1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "Summary\n\n\nDefinition of Remote Sensing\n\nAccording to NASA, remote sensing refers to information obtained at a distance. These sensors are placed on satellites or aircrafts and they detect and document reflected or emitted energy.\n\nWhat kinds of energy?\n\nTO cut it short, the answer is Electromagnetic Radiation (EMR). This energy travels in different forms of waves through the atmosphere. While human eyes only detects visible light, the sensors can utilise the full range of the electromagnetic spectrum to collect data.\n\n\n\n\n\n\n\n\n\nElectromagnetic Spectrum (Source: NASA Science)\n\nSensor Types\n\nThere are two types of remote sensors: active and passive sensors.\n\nActive sensor:\n\nemits electromagnetic energy and receives the reflected energy\ncan observe areas under most conditions as most active sensors operate in the microwave band of the electromagnetic spectrum\nrequires power source - solar energy\naffected by space weather - solar flares\n\nPassive sensor:\n\nusually detects reflected energy\nused for measuring physical attributes, such as land/sea surface temperature and vegetation cover\nhas limitations in observing areas in the presence of dense cloud cover\n\n\n\n\n\n\n\n\n\n\n\nPassive and Active Sensors (Source: NASA)\n\nDoes EMR interact with other factors?\n\nYes, these radiations are often influenced by Earth’s surface and atmospheric conditions, which might distort the original information.\n\n\n\n\n\n\n\n\n\nEMR’s interaction with surface and atmosphere (Source: Daneshgar, 2015)\n\n4 Resolutions of Remote Sensing Data\n\n\nSpatial resolution: sizes of the raster cells\nSpectral resolution:\n\nValues for each wavelength across the electromagnetic spectrum creates a spectral signature\nEvery object has its own unique spectral signature, thus it can be used for identifying a specific object\nBut spectral resolution is often affected by atmospheric particles which absorb parts of the spectrum\n\n\n\n\n\n\n\n\n\n\n\nAtmospheric Electromagnetic Opacity (Source: GIS Geography)\n\nTemporal resolution: frequency of the recorded data\nRadiometric resolution: sensor’s ability to detect subtle differences in energy which determines the quality of images\n\n\nApplication\n\nIn the context of climate change, the ability to map tree types are important as they are closely linked to biodiversity. As mentioned earlier, each feature has different spectral signatures. Durgante et al. (2013) have discovered that spectral signature of tree species was better than DNA identification in distinguishing tree species. The researchers used ‘Fourier-Transform Near-Infrared (FT-NIR)’. It obtained the best results of 99.4% of correct specimen identification when using 36 spectral readings per specimen. This somehow opens a new, cost- and time-efficient avenue in mapping forest resources. Their research is meaningful in a sense that it enables us to identify types of forests in which forests are not easily accessible by humans or should be intact from human interferences.\n\n\n\n\n\n\n\n\n\nSpectral signatures of each tree (Source: Durgante et al., 2013)\nThe potential of spectral signature can also be found in the business sector. Yang, Lee, and Williamson (2012) suggested that spectral signatures could be employed to blueberry yield estimation system. The research team confirmed six-class blueberry classification - 233, 551, 554, 691, 699, 1373 nm - yielded the best results. This showed the potential of the spectral signature in developing fast and low cost blueberry detector. Furthermore, this research expands the use of spectral signatures from academia to industry, and suggests that the use of spectral signature can be boundless.\n\n\n\n\n\n\n\n\n\nDifferent absorbance rate depending on blueberry’s growth (Source: Yang and Williamson, 2012)\n\nPersonal Reflection\n\nAs a person who is interested in urban green spaces, the spectral resolution was an interesting concept. After understanding how it can be applied in both academia and industry, I wondered whether the health of green spaces in a city can also be measured with it. Furthermore, I was wondering whether there are any methods that improve the quality of data obtained from passive sensors as they cannot obtain a good quality data in the presence of bad weather. Under the circumstances where we have passive sensors, are there any techniques that could correct images?\n\nReferences\n\nNASA EarthData. (n.d.) https://www.earthdata.nasa.gov/ (Accessed: 26.01.2023)\nGIS Geography. (2022) https://gisgeography.com/atmospheric-window/ (Accessed: 26.01.2023)\n\n\n\n\nDurgante, Flávia Machado, Niro Higuchi, Ana Almeida, and Alberto Vicentini. 2013. “Species Spectral Signature: Discriminating Closely Related Plant Species in the Amazon with Near-Infrared Leaf-Spectroscopy.” Forest Ecology and Management 291: 240–48. https://doi.org/https://doi.org/10.1016/j.foreco.2012.10.045.\n\n\nYang, Ce, Won Suk Lee, and Jeffrey G. Williamson. 2012. “Classification of Blueberry Fruit and Leaves Based on Spectral Signatures.” Biosystems Engineering 113 (4): 351–62. https://doi.org/https://doi.org/10.1016/j.biosystemseng.2012.09.009."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  Week 2",
    "section": "",
    "text": "https://phily5051.github.io/CASA0023_wk2_slides/#1"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Durgante, Flávia Machado, Niro Higuchi, Ana Almeida, and Alberto\nVicentini. 2013. “Species Spectral Signature: Discriminating\nClosely Related Plant Species in the Amazon with Near-Infrared\nLeaf-Spectroscopy.” Forest Ecology and Management 291:\n240–48. https://doi.org/https://doi.org/10.1016/j.foreco.2012.10.045.\n\n\nYang, Ce, Won Suk Lee, and Jeffrey G. Williamson. 2012.\n“Classification of Blueberry Fruit and Leaves Based on Spectral\nSignatures.” Biosystems Engineering 113 (4): 351–62.\nhttps://doi.org/https://doi.org/10.1016/j.biosystemseng.2012.09.009."
  },
  {
    "objectID": "week_4.html",
    "href": "week_4.html",
    "title": "3  Week 4",
    "section": "",
    "text": "Summary\n\nIn this diary, I am going to talk about the biggest city plan in South Korea and how a low-level city plan guideline is aligned with its upper-level plan. In particular, I will focus on the implementation of the plan regarding environment and whether there is any implementation that can be benefited from using remote sensing.\n\nBrief Information about 2030 Seoul Plan\n\n2030 Seoul Plan is the highest level city plan in Seoul, South Korea. Provided that Seoul is a capital city in South Korea, its city plan is often considered as significant and determinant as a government-led plan, in terms of its impacts on citizens of Seoul and the country. The plan covers a number of sectors which include housing, transport and environment. The city plan presents Seoul’s future vision and outlines the city’s development plan for the next 10 years.\nWith regards to environment, the plan specifies 3 Targets and 11 Strategies. The 3 targets are as follows:\n\nTarget 1: Ecological city with numerous urban parks\nTarget 2: Energy-efficient city\nTarget 3: Disaster-free city\n\nAmong the illustrated targets, Strategy 1-4 under Target 1 suggests a number of ways to deal with particulate matter, which is one of the biggest environmental issues in South Korea and highly related to citizen’s life. One of the mitigation actions to deal with the air pollutants identified by the Seoul government is creating urban forests on those areas which are specifically vulnerable to fine dust.\n\n\n\n\n\n\n\n\n\n2030 Seoul Plan (Source: Seoul Metropolitan Government)\n\nUrban Forest Guideline for PM Mitigation\n\nUrban Forest Guideline for PM Mitigation is a low-level city plan which entails types of activities that will be executed in line with the 2030 Seoul Plan. The guideline specified a number of technical rules to follow regarding types of trees to plant, which areas are targeted for urban forests implementation, and suitable distance between each tree stand and so on.\nFurthermore, the guideline categorised six types of urban forests that could mitigate the concentration of PM. These are street trees, urban parks, forests around the city, vegetations in schools and river, and green spaces within proximity to residential areas.\n\n\n\n\n\n\n\n\n\nDistance between trees in residential areas (Source: Seoul Metropolitan Government)\n\n3 Functional Urban Forest Types for PM Mitigation\n\nThe said urban forest types can be grouped into 3 types of urban forests in terms of their objective and functionality. These are PM blockage urban forests, PM mitigation urban forests and Wind corridor urban forests.\n\nPM blockage urban forests:\n\ndense urban forests with 1,800 trees per ha\nto block PM diffusion\n\nPM mitigation urban forests:\n\nbetween 800 and 1,000 trees per ha\nto enhance PM absorption by urban forests\n\nWind corridor urban forests:\n\n500 trees per ha\nto introduce clean air to city centre\nto enable the outflow of congregated PM within the city due to vehicles and household heating.\n\n\n\n\n\n\n\n\n\n\n\n3 Types of PM Mitigation Urban Forest Types (Source: Korea Forest Service)\n\nApplication\n\nRemote sensing can come into play a key role in identifying what PM mitigation urban forest types are most suitable to a particular site within a city, and quantifying the amounts of PM absorbed by urban forests.\nRemote sensing is an effective measure to map and monitor PM concentrations (Zhang et al. 2021). This indicates that areas with routinely high PM concentrations can be detected by satellite images. Thus, government officials could take data-informed decisions and select the most suitable PM mitigation urban forest types for each particular area.\n\n\n\n\n\n\n\n\n\nAverage PM2.5 concentration between 2001 and 2006 mapped by remote sensing (Source: Zhang et al.)\nIn addition, Gupta et al. (2006) found that remote sensing can capture the movement of air pollutants over a large area as satellite measurements are available globally. Their findings allude to us that remote sensing could enable us to understand how much PM have been absorbed by urban forests by measuring the size of PM mass between areas. The policymakers could draw on these quantified data as a basis for future policies.\n\n\n\n\n\n\n\n\n\nDocumentation of 1-year change of PM2.5 mass concentration in five study regions (Source: Gupta et al.)\n\nLink to UN SDG Goals\n\nAccording to United Nations 2030 Sustainable Development Goals (SDGs), the use of remote sensing could contribute to achieving 2 SDGs goals.\n\nSDG 3. Good Health and Well-Being\n\nPM causes various health problems, such as lung and heart diseases. The reduction in PM could improve citizen’s health and quality of life.\n\nSDG 11. Sustainable Cities and Communities\n\nCreating urban forests, as a measure to mitigate PM, could not only reduce aereal PM but also contribute to building a safe and resilient built environment from other natural hazards.\n\n\n\nReflection\n\nThis week’s contents were somewhat different from what we have learnt past few months. Understanding policy and how remote sensing could come into play its part in realising the policy’s objective was interesting. The contemplation upon how remote sensing can be applied at a particular context and a site broadened my understanding. Of course, I am aware that remote sensing is one of many solutions that could assist in achieving policy’s goals, and 1-size-fits-all approach is not sufficient to solve similar problems in different contexts. Thanks to this week’s lecture, however, I could say that I came to understand how we can apply our theoretical and technical knowledge about remote sensing to solve the real problems.\n\nReferences\n\nKorea Forest Service. (n.d.) https://english.forest.go.kr/kfsweb/kfs/subIdx/Index.do?mn=UENG (Accessed: 02.02.2022)\nSeoul Metropolitan Government. (n.d.) https://www.seoul.go.kr/main/index.jsp (Accessed: 02.02.2022)\nUnited Nations. (n.d.) https://sdgs.un.org/goals (Accessed: 02.02.2022)\n\n\n\n\nGupta, Pawan, Sundar A. Christopher, Jun Wang, Robert Gehrig, Yc Lee, and Naresh Kumar. 2006. “Satellite Remote Sensing of Particulate Matter and Air Quality Assessment over Global Cities.” Atmospheric Environment 40 (30): 5880–92. https://doi.org/https://doi.org/10.1016/j.atmosenv.2006.03.016.\n\n\nZhang, Ying, Zhengqiang Li, Kaixu Bai, Yuanyuan Wei, Yisong Xie, Yuanxun Zhang, Yang Ou, et al. 2021. “Satellite Remote Sensing of Atmospheric Particulate Matter Mass Concentration: Advances, Challenges, and Perspectives.” Fundamental Research 1 (3): 240–58. https://doi.org/https://doi.org/10.1016/j.fmre.2021.04.007."
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3",
    "section": "",
    "text": "3.1 Summary\n\n\nCauses of Remote Sensing Data Distortion\n\nRemotely sensed images often require corrections to be available for analysis. The image distortions occur due to the following factors:\n\nView angle: depending on the angle of the sensor, the area of interest might look different.\n\n\n\n\n\n\n\n\n\n\nView angle (Source: Shen et al., 2021)\n\nTopography: the shape of terrain might cause some flaws in the image.\n\n\n\n\n\n\n\n\n\n\nThe impact of topography on remote sensing images (Source: Julia Lenhardt)\n\nEarth rotation: Earth’s spinning motion poses another difficulty. Straight lines of an image can appear to be unnaturally curved or deformed.\n\n\n\n\n\n\n\n\n\n\nEffect of Earth’s motion (Source: National Learning Network for Remote Sensing)\n\nWind, and so on.\n\n\nData Corrections\n\n4 types of data correction methods are available for distorted images. These are Geometric, Atmospheric, Orthorectification corrections!\nFirst, we can kick off with Geometric Correction!\n\nGeometric Correction\n\nGeometric errors are normally introduced by internal and external factors (attitude and altitude change). And Ground Control Points (GPS) can be used for correcting the errors! This means, basically, we are identifying so-called “known points” in both sensed image and a reference data and use them to build regression models! But we need each GCP, which are “image coordinate” and “map coordinate”.\n\n\n\n\n\n\n\n\n\nGPS points from a map and Landsat image (Source: GeoLearn)\n\nSolution:\n\nLinear Regression in a “Backward mapping” fashion can be an answer to this problem. Backward mapping is going from output image to input image. This method goes through one pixel at a time and for each position in output image to calculate where in the input image a pixel must come from.\nThe result can be verified by using RMSE. The lower the value is, the more accurate the result is, and the threshold RMSE value is 0.5. To reduce the RMSE, more GPS can be added as well.But when we do this, we have to re-sample the final raster data. There are several resample methods, such as Nearest Neighbour, Linear, Cubic and Cubic spline.\n\n\n\n\n\n\n\n\n\nGeometric Correction (Source: Abdul Basith)\n\n\n\n\n\n\n\n\n\nNearest Neighbour Resampling method (Source: AWF-WIKI)\n\nAtmospheric Correction\n\nAtmospheric correction removes scattering and absorption effects of the atmosphere to improve the image. Absorption and scattering makes an image somewhat blurry as they reduce contrast of image.\n\n\n\n\n\n\n\n\n\nExample of an image before and after atmospheric correction (Source: NASA)\n\nSolution: there are two kinds of approaches - relative and absolute.\n\nRelative atmospheric correction typically consists of two methods.\n- Dark Object Subtraction (DOS): searches each band for the darkest pixel value. The scattering is removed by subtracting this value from every pixel in the band.\n\n\n\n\n\n\n\n\n\nAn improved image after DOS (Source: Wang et al., 2019)\n- Pseudo-Invariant Features (PIFs): is grounded upon statistical invariance of artificial elements, which do not exhibit seasonal variation. The differences in PIFs reflectance between dates are assumed to be due to atmospheric conditions and are linearly related.\n\n\n\n\n\n\n\n\n\nAn improved image after employing PIFs (Source: Schott et al., 1988)\nRelative atmospheric correction convert digital brightness values into scaled surface reflectance by using atmospheric radiative transfer models.\n\n\n\n\n\n\n\n\n\nBefore and after atmospheric correction using radiative transfer method (Source: Advanced Remote Sensing)\n\nOrthorectification Correction\n\nThis method removes distortions by making the images looked right above (at nadir). This correction necessiates sensor geometry and an elevation model.\n\n\n\n\n\n\n\n\n\nAn example of orthorectification correction (Source: Esri Insider, 2016)\n\nData Join\n\nJoining remote sensed images are coined as “Mosaicking”. Using histogram matching algorithm, similar brightness values are given to two images that are overlapped, and then the feathering is conducted.\n\n\n\n\n\n\n\n\n\nSix raster datasets are joined into one image (Source: ArcMap)\n\nImage Enhancement\n\nMaterials reflect different amounts of energy in the same wavelengths. However, sensors have a fixed range of Digital Number (DN) values between 4 to 105. If we expand this range, we could enhance the visual appearance of images. There are several methods to do this, which include Minimum-Maximum, Percentage Linear and Standard Deviation, and Piecewise Linear Contrast Stretch.\n\nMinimum-Maximum: utilises the full range of available brightness values\n\n\n\n\n\n\n\n\n\n\nMinimum-Maximum Linear Stretch (Source: OneStopGIS)\n\nPercentage Linear and Standard Deviation Stretch: is similar to the Minimum-Maximum linear contrast stretch but this method uses a specified minimum and maximum values that situate between a certain percentage of pixels from the mean of the histogram.\n\n\n\n\n\n\n\n\n\n\nPercentage Linear Contrast Stretch (Source: OneStopGIS)\n\nPiecewise Linear Contrast Stretch: is used when the distribution of a histogram in an image is bi or trimodal (Jensen and Schill, n.d.).\n\n\n\n\n\n\n\n\n\n\nPiecewise Linear Contrast Stretch (Source: Tsai and Yeh, 2008)\n\n3.2 Application\n\nImage enhancement techniques not only enhance visual appearance but also allows for comparison between images retrieved from different years.\nHan-qiu (2011) has attempted to compare thermal images from different years with the same date in order to investigate urban heat island (UHI). The thermal infrared bands of different date were processed with several image enhancement technologies.\nTo detect change of the UHI between different dates, the two thermal imageries were normalised and scaled to reduce the impacts caused by seasonal variations or weather conditions. Then, the images were overlaid to show difference between images by subtracting correponding pixels.\nThis research demonstrates how we can employ image enhancement technologies to compare images from different years in a quantitative fashion.\nIn addition to image enhancement technologies covered in the lecture, Hu et al. (2020) demonstrated that Convolutional Neural Networks (CNN) can enhance visual appearance of image greatly. The authors carried out performing several different image enhancement technologies on low-quality image.\n\n\n\n\n\n\n\n\n\nResults of different image enhancement methods (Source: Hu et al., 2020)\nTheir findings suggest remote-sensing CNN (RSCNN) has proven to be the most efficient method in enhancing visualisation effects and allowing a better interpretation. RSCNN showed the superiority in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise_Ratio (PSNR) over conventional techniques.\n\n\n\n\n\n\n\n\n\nSSIM and PSNR of different image enhancement methods (Source: Hu et al., 2020)\nThis research somehow offered potentials in combining two different fields of studies - CNN and remote sensing -, and suggested that adding complexity to imagery can help us more natural results with more realistic textures and vivid details.\n\n3.3 Reflection\n\nThis week’s content gave me an answer for the questions I had from the first week. Numerous data correction technologies improve the quality of image collected and allow for facilitating data to be ready for analysis.\nHowever, as there are a variety of correction methods, it seems necessary to understand what causes flaws in the remotely sensed images and how to employ an appropriate correction method to that specific datasets. In addition, I wonder whether there is a preference in a particular correction method over other correction methods, depending on the domain of study field or industry.\nLastly, as we went through numerous technologies which enable us to identify objects or areas in a better and higher resolution. However, in the context of built environment, there are many objects in cities that look similar to each other, and this might pose a difficulty in classifying them.\nIn Data Science module, I came across a deep learning model called YOLOv5, which detects objects in remotely sensed images. The model was trained on custom datasets. Once the training is done, the model can be applied to satellite imagery of different resolutions and identify objects.\n\n\n\n\n\n\n\n\n\nObjects indicated by a bounding box and its probability (Source: Google Earth Engine for OSINT)\nIf we create a customised dataset which contains specific information about some types of urban infrastructure, and feed these to train a model, we might be able to build a deep learning object detection model targeted at urban environment. Together with the data correction and image enhancement methods we learnt, I think the object detection model could enable us to classify some technically-challenging objects in an urban setting.\n\n3.4 References\n\nGoogle Earth Engine for OSINT. (n.d.) ‘Object Detection in Satellite Imagery’[Online]. Available at: https://oballinger.github.io/GEE_OSINT/object_detection.html (Acessed: 08.02.2023)\nJensen, J.H. and Schill, S.R. (n.d.) ‘Contrast Enhancement’[PDF]. Available at: http://knightlab.org/rscc/legacy/RSCC_Contrast_Enhancement.pdf (Accessed: 08.02.2023)\n\n\n\n\nHan-qiu, Chen. 2011. “An Image Processing Technique for the Study of Urban Heat Island Changes Using Different Seasonal Remote Sensing Data.” Remote Sensing Technology and Application 18: 129–33.\n\n\nHu, Linshu, Mengjiao Qin, Feng Zhang, Zhenhong Du, and Renyi Liu. 2020. “RSCNN: A CNN-Based Method to Enhance Low-Light Remote-Sensing Images.” Remote Sensing 13 (December): 62. https://doi.org/10.3390/rs13010062."
  }
]