---
title: "Classification II"
bibliography: references.bib
editor: visual
---

## 7.1 Summary

In this section, I am going to give an overall idea of how Machine Learning (ML) classification algorithm works, explain the basic concept of sub pixel analysis and how to assess the accuracy of the models we build.

### 7.1.1 Remote Sensing ML Classification Methods In a Nutshell

-   **Building a ML model in Remote Sensing**: I drew the image below to illustrate the general idea of how ML models are built in remote sensing. The pixel values are extracted from [Region of Interest (ROI)]{.underline}, and the data are stored in a tabular format. Each row contains the values of each pixel. The output value (land cover type) that we are trying to predict is called '[Label]{.underline}', and the input data are called '[Features]{.underline}'. 70% of the values are used for training a model, the rest is withheld for testing the accuracy of the model.

```{r echo=FALSE, out.width="80%", out.height="40%", fig.align='center', fig.cap= "ML Classification Model in a Nutshell"}
knitr::include_graphics('img/drawing.jpg')
```

-   **Any considerations?**:

    -   For generalizability, better use [multi-year data]{.underline}, rather than data from one single year
    -   Understanding of [spatial autocorrelation]{.underline}
    -   Hard vs Soft classification

-   *A bit of thought.....*

    So far, the classification methods we covered are at a per pixel approach, which assumes that a pixel contains only one land type. However, in reality, this is hardly true as each pixel can consist of different types of land. If we are doing a research, don't we need to set [detailed criteria or reasons to say a certain pixel a specific land type]{.underline} when building a model and share this with people?

### 7.1.2 Spectral Mixture Analysis (SMA)

-   **Definition**: SMA is a sub-pixel analysis technique which [estimates the proportion of different land covers within a pixel based on their spectral signatures]{.underline} [@manolakislockwoodcooley2016].

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align='center', fig.cap= "A pixel of 3 mixture components (Source: [Machado and Small, 2013](https://www.researchgate.net/figure/Perfect-decomposition-with-a-Linear-Spectral-Mixture-Model-LSMM-on-a-30-m-pixel-formed_fig6_259715697))"}
knitr::include_graphics('img/sma.png')
```

-   **Assumption**: [The reflectance spectrum of a pixel is a linear sum of the reflectance spectra of Endmembers]{.underline} (different land cover types) present within that pixel [@articlea].

    *Note*. [Endmembers]{.underline} are spectrally-pure pixels which have homogeneous land cover, and there are typically only a few of them.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align='center', fig.cap= "The V-I-S (Vegetation-Impervious surface-Soil) model of Ridd (1995) (Source: [Le≈õko, 2013](https://www.researchgate.net/publication/312194814_Comparison_of_Eastern_and_Western_Europe_spatial_development_of_cities_based_on_Remote_sensing_data))"}
knitr::include_graphics('img/vis.jpg')
```

-   *Any considerations?*:

    -   Number of Endmembers
    -   Pixel purity

-   *A bit of thought.....*

    Can we really tell that [Endmembers are representative of each land cover type?]{.underline} In previous lecture on signature reflectance, there was disparities in signature reflectance even between the same land cover type. For example, different tree species had different signature reflectance.

### 7.1.3 Accuracy Assessment

1.  **Error matrix**:

    -   [Producer accuracy (Recall)]{.underline}:

        -   How often are a map maker's classification of real features on the ground is correct
        -   $TP$ / $TP$ + $FN$

    -   [User accuracy (Precision)]{.underline}:

        -   How often the class on the map will actually be present on the ground
        -   $TP$ / $TP$ + $FP$

    -   [Overall accuracy (OA)]{.underline}:

        -   How well a map accurately represents all the reference sites
        -   $TP$ $+$ $TN$ / $TP + FP + FN + TN$

2.  **F1**:

    -   Combines both recall and precision
    -   $F1 = 2 * Precision * Recall / Precision + Recall$

## 7.2 Application

All ML models need to be tested with withheld data to avoid over-fitting issues. When it comes to ML classification models in remote sensing, however, this is not enough!. There is another factor that we have to consider...

**Spatial autocorrelation**!!

@tobler argued the first law of geography is that [near things are more closely related than distant things]{.underline}. Even if we split our spatial data into training and test data, there will always be "possibilities of spatial autocorrelation".

### 7.2.1 Spatial dependence between training and test sets: another pitfall of classification accuracy assessment in remote sensing

-   **Summary**: @karasiak discuss [how spatial autocorrelation affects remote sensing image classification and emphasizes the need to account for spatial independence between training and test sets to avoid overestimating the model's generalization capabilities.]{.underline} The authors built a random forest (RF) model to classify forest stands. The research team compares the overall accuracy (OA) of six cross-validation (CV) strategies with different sample sizes - pixel and object levels, and demonstrates that spatial leave-one-out cross-validation is the most effective approach to obtaining unbiased estimates of predictive error.

-   **Main Findings**:

    -   Ignoring spatial autocorrelation [resulted in a high accuracy - overfitting-]{.underline} in both sample levels

    -   [Non-spatial LOO or *k* - fold CV at the object level can mitigate the effect of spatial autocorrelation, when the number of sample size is not too large]{.underline} (50 forest stands)

        -   If the number of sample increases (over 500 forest stands), the distance between forest stands reduces, leading to increased effect of spatial dependence between training and test datasets

    -   [Spatial LOO CV is a better choice]{.underline}

-   **Comment**:

    This paper emphasises [the importance of spatial autocorrelation and spatial independence between training and test datasets in accuracy assessment]{.underline}. The authors show that traditional non-spatial cross-validation methods can lead to overestimating accuracy, and spatial cross-validation provides better estimates of predictive error.

-   **Limitation**: However, the paper has [limitations in terms of its generalizability to other types of remote sensing imagery and classification tasks since it only focuses on forest classification using Sentinel-2 data]{.underline}. Furthermore, the authors did not provide a thorough analysis of the proposed approach's limitations and the possible trade-offs between accuracy and computational efficiency. Further research is necessary to validate the proposed approach in various contexts and to investigate its potential drawbacks and trade-offs.

```{r echo=FALSE, out.width= "60%", out.height= "40%", fig.show='hold', fig.align= 'center', fig.cap= "Average OA accuracy for each CV strategies (Source: [Karasiak et al, 2021](https://www.researchgate.net/publication/351106376_Spatial_dependence_between_training_and_test_sets_another_pitfall_of_classification_accuracy_assessment_in_remote_sensing))"}
knitr::include_graphics('img/rs_cv.jpg')
```

```{r echo=FALSE, out.width= "60%", out.height= "40%", fig.show='hold', fig.align= 'center', fig.cap= "Learning curve for each CV strategies (Source: [Karasiak et al, 2021](https://www.researchgate.net/publication/351106376_Spatial_dependence_between_training_and_test_sets_another_pitfall_of_classification_accuracy_assessment_in_remote_sensing))"}

knitr::include_graphics('img/rs_learningcurve.jpg')
```

## 7.3 Reflection

This week's contents helped me understand the particular characteristics of remote sensing ML models. As ML models are grounded on spatial data, spatial autocorrelation is always inherent in the data we use.

Building a ML model without understanding this distinctive feature would result in a misleading classification.

Moreover, I was pleased to see what we learnt in the previous term (**Moran's I**) comes into play in solving spatial autocorrelation in ML models. This reinforced my understanding of previous lessons as well as connected it to new concepts and methodologies.

*A bit of thought.....*

-   The object-based non-spatial CV is just an alternative of spatial CV when the sample size is not too large.

-   If we are classifying land cover types other than forest stands, would this optimal sample number be different from the forest stands? What is the optimal number of sample sizes that we are safe to use non-spatial CV to assess remote sensing ML model?

In closing this week's diary, I would like to look into any other new ML models in remote sensing which are not covered in the lecture, and their usage in urban environments.
